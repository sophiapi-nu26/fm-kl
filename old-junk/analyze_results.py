#!/usr/bin/env python3
"""
Script to analyze NPZ results from KL evolution identity validation.

This script loads the NPZ files generated by the experiment and provides
comprehensive analysis of the results.
"""

import numpy as np
import json
import matplotlib.pyplot as plt
import os
from pathlib import Path

def load_evaluation_results(npz_path):
    """
    Load evaluation results from NPZ file.
    
    Args:
        npz_path: Path to the NPZ file
        
    Returns:
        Dictionary containing all results
    """
    print(f"Loading results from: {npz_path}")
    
    with np.load(npz_path) as data:
        print(f"  Available keys: {list(data.keys())}")
        for key in data.keys():
            print(f"  {key}: type={type(data[key])}, shape={data[key].shape if hasattr(data[key], 'shape') else 'N/A'}")
        results = {}
        for key in data.keys():
            if key == 'error_stats':
                # Handle both JSON string and direct array cases
                error_stats_data = data[key]
                if isinstance(error_stats_data, np.ndarray):
                    # If it's a scalar (shape=()), it's likely a JSON string stored as bytes
                    if error_stats_data.shape == ():
                        try:
                            # Try to decode as JSON string
                            json_str = error_stats_data.item()
                            print(f"    error_stats content: {json_str} (type: {type(json_str)})")
                            if isinstance(json_str, str):
                                results[key] = json.loads(json_str)
                            else:
                                # If it's bytes, decode first
                                json_str = json_str.decode('utf-8') if isinstance(json_str, bytes) else str(json_str)
                                results[key] = json.loads(json_str)
                        except Exception as e:
                            print(f"    Error parsing error_stats: {e}")
                            # Fallback: create dummy stats
                            results[key] = {'median': 0.0, 'mean': 0.0, 'max': 0.0}
                    else:
                        # If it's a regular array, convert to dict
                        results[key] = {
                            'median': float(error_stats_data[0]) if len(error_stats_data) > 0 else 0.0,
                            'mean': float(error_stats_data[1]) if len(error_stats_data) > 1 else 0.0,
                            'max': float(error_stats_data[2]) if len(error_stats_data) > 2 else 0.0
                        }
                else:
                    # If it's a JSON string, parse it
                    results[key] = json.loads(error_stats_data)
            elif key == 'elapsed_time':
                results[key] = float(data[key])
            else:
                results[key] = data[key]
    
    return results

def analyze_results(results, schedule_type):
    """
    Analyze the evaluation results and print summary statistics.
    
    Args:
        results: Dictionary containing evaluation results
        schedule_type: Type of schedule (e.g., 'linear', 'sin_pi')
    """
    print(f"\n{'='*60}")
    print(f"ANALYSIS FOR SCHEDULE: {schedule_type.upper()}")
    print(f"{'='*60}")
    
    # Basic info
    print(f"Time points: {len(results['t_grid'])}")
    print(f"Computation time: {results['elapsed_time']:.2f} seconds")
    
    # Error statistics
    error_stats = results['error_stats']
    print(f"\nError Statistics:")
    print(f"  Median relative error: {error_stats['median']:.6f}")
    print(f"  Mean relative error: {error_stats['mean']:.6f}")
    print(f"  Max relative error: {error_stats['max']:.6f}")
    
    # KL divergence analysis
    kl_estimator = results['kl_estimator']
    rhs_cumulative = results['rhs_cumulative']
    relative_error = results['relative_error']
    
    print(f"\nKL Divergence Analysis:")
    print(f"  KL estimator range: [{kl_estimator.min():.6f}, {kl_estimator.max():.6f}]")
    print(f"  RHS cumulative range: [{rhs_cumulative.min():.6f}, {rhs_cumulative.max():.6f}]")
    print(f"  Mean KL estimator: {kl_estimator.mean():.6f}")
    print(f"  Mean RHS cumulative: {rhs_cumulative.mean():.6f}")
    
    # Relative error analysis
    print(f"\nRelative Error Analysis:")
    print(f"  Error range: [{relative_error.min():.6f}, {relative_error.max():.6f}]")
    print(f"  Points with error < 0.01: {np.sum(relative_error < 0.01)}/{len(relative_error)}")
    print(f"  Points with error < 0.05: {np.sum(relative_error < 0.05)}/{len(relative_error)}")
    print(f"  Points with error < 0.10: {np.sum(relative_error < 0.10)}/{len(relative_error)}")
    
    # Time analysis
    t_grid = results['t_grid']
    print(f"\nTime Analysis:")
    print(f"  Time range: [{t_grid.min():.3f}, {t_grid.max():.3f}]")
    print(f"  Time step: {(t_grid.max() - t_grid.min()) / (len(t_grid) - 1):.3f}")

def plot_results(results, schedule_type, save_plots=True):
    """
    Create plots of the results.
    
    Args:
        results: Dictionary containing evaluation results
        schedule_type: Type of schedule
        save_plots: Whether to save plots to file
    """
    t_grid = results['t_grid']
    kl_estimator = results['kl_estimator']
    rhs_cumulative = results['rhs_cumulative']
    relative_error = results['relative_error']
    
    # Create figure with subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), 
                                  gridspec_kw={'height_ratios': [3, 1]})
    
    # Main plot: KL curves
    ax1.plot(t_grid, kl_estimator, 'b-', linewidth=2, label='KL̂(t)', alpha=0.8)
    ax1.plot(t_grid, rhs_cumulative, 'r--', linewidth=2, label='R(t)', alpha=0.8)
    ax1.fill_between(t_grid, kl_estimator, rhs_cumulative, alpha=0.2, color='gray')
    
    ax1.set_xlabel('Time t')
    ax1.set_ylabel('KL Divergence')
    ax1.set_title(f'KL Evolution Identity Validation - {schedule_type}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, 1)
    
    # Relative error plot
    ax2.semilogy(t_grid, relative_error, 'g-', linewidth=1.5, alpha=0.8)
    ax2.axhline(y=0.03, color='orange', linestyle=':', alpha=0.7, label='3% threshold')
    ax2.axhline(y=0.08, color='red', linestyle=':', alpha=0.7, label='8% threshold')
    
    ax2.set_xlabel('Time t')
    ax2.set_ylabel('Relative Error')
    ax2.set_title('Relative Error: |R(t) - KL̂(t)| / max(10⁻⁸, KL̂(t))')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(0, 1)
    
    # Add error statistics text
    error_stats = results['error_stats']
    stats_text = (f"Median: {error_stats['median']:.3f}\n"
                 f"Mean: {error_stats['mean']:.3f}\n"
                 f"Max: {error_stats['max']:.3f}")
    ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, 
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    if save_plots:
        plot_path = f"analysis_{schedule_type}_results.pdf"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to: {plot_path}")
    
    plt.show()

def main():
    """Main function to analyze all available results."""
    results_dir = Path("results")
    
    if not results_dir.exists():
        print("Results directory not found!")
        return
    
    # Find all NPZ files
    npz_files = list(results_dir.glob("evaluation_*.npz"))
    
    if not npz_files:
        print("No evaluation NPZ files found!")
        return
    
    print(f"Found {len(npz_files)} evaluation files:")
    for file in npz_files:
        print(f"  - {file.name}")
    
    # Analyze each file
    for npz_file in npz_files:
        try:
            # Extract schedule type from filename
            schedule_type = npz_file.stem.replace("evaluation_", "")
            
            # Load results
            results = load_evaluation_results(npz_file)
            
            # Analyze results
            analyze_results(results, schedule_type)
            
            # Create plots
            plot_results(results, schedule_type, save_plots=True)
            
        except Exception as e:
            print(f"Error analyzing {npz_file.name}: {e}")
            continue
    
    print(f"\n{'='*60}")
    print("ANALYSIS COMPLETE")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
